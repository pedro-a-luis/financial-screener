# Financial Screener - Project Overview

**Last Updated:** 2025-10-28
**Status:** Active Development - Phase 2 (Data Collection Infrastructure)
**Environment:** 8-node Raspberry Pi 4 (ARM64) K3s cluster
**Tech Stack:** Python 3.11, PostgreSQL 14 + TimescaleDB, Airflow 3.1.0, Kubernetes

---

## Quick Start

### Access Points
- **Airflow UI:** http://192.168.1.240:30800 (NodePort)
- **PostgreSQL:** localhost:5432 (via SSH tunnel from 192.168.1.240:5432)
- **Cluster Master:** ssh -i ~/.ssh/pi_cluster admin@192.168.1.240

### Key Commands
```bash
# SSH to cluster
ssh -i ~/.ssh/pi_cluster admin@192.168.1.240

# Check Airflow status
kubectl get pods -n airflow

# Check data collector pods
kubectl get pods -n financial-screener

# View database (requires tunnel)
systemctl status postgres-tunnel.service
psql -h localhost -U appuser -d appdb

# Trigger DAG manually
kubectl exec -n airflow airflow-scheduler-0 -c scheduler -- \
  airflow dags trigger data_collection_equities

# View recent API quota status
psql -h localhost -U appuser -d appdb -c \
  "SET search_path TO financial_screener; SELECT * FROM v_quota_status WHERE quota_date = CURRENT_DATE;"
```

---

## System Architecture

### Infrastructure
```
WSL2 (Development)
  └─> SSH tunnel to Raspberry Pi Cluster (192.168.1.240-247)
       ├─> K3s (Lightweight Kubernetes for ARM64)
       ├─> PostgreSQL 14 + TimescaleDB (namespace: databases)
       ├─> Apache Airflow 3.1.0 (namespace: airflow)
       └─> Data Collector Jobs (namespace: financial-screener)
```

### Data Flow
```
Airflow Scheduler (Daily 21:30 UTC)
  ├─> initialize_execution → Create process_executions record
  ├─> discover_delta → Query v_assets_needing_processing
  ├─> check_api_quota → Validate quota, adjust batch sizes
  ├─> Parallel Processing (4 jobs):
  │    ├─> process_us_markets (NYSE + NASDAQ)
  │    ├─> process_lse (London Stock Exchange)
  │    ├─> process_german_markets (Frankfurt + Xetra)
  │    └─> process_european_markets (Euronext + BME + SIX)
  │         └─> BashOperator + kubectl run → Data Collector Pods
  │              └─> EODHD API → PostgreSQL (financial_screener schema)
  ├─> finalize_execution → Update process_executions
  └─> trigger_indicators → Start indicator calculation DAG
```

---

## Database Schema

### Schema: financial_screener

#### Core Tables
1. **assets** (21,817 rows)
   - 72 columns + 13 JSONB fields
   - Stores ticker metadata, fundamentals, company info
   - Partitioned by exchange (future enhancement)

2. **historical_prices** (TimescaleDB hypertable)
   - Partitioned by (ticker, time) - 7-day chunks
   - OHLCV + adjusted values
   - ~5.4M rows per full dataset

3. **technical_indicators**
   - SMA, EMA, RSI, MACD, Bollinger Bands
   - Calculated from historical_prices

#### Metadata Tables (Process Orchestration)
4. **process_executions**
   - Tracks every DAG run
   - Stores execution parameters, results, API usage

5. **asset_processing_state**
   - State machine for each ticker (21,817 rows)
   - Tracks fundamentals_loaded, prices_last_date, indicators_calculated
   - Drives delta discovery

6. **asset_processing_details**
   - Granular log of each operation on each ticker
   - ~8M rows/year expected

#### API Quota Management (NEW - 2025-10-28)
7. **api_usage_log**
   - Every API call logged with cost
   - Supports quota tracking and debugging

8. **api_daily_quota**
   - Daily quota limits and usage counters
   - Real-time tracking of 100K daily limit

9. **api_ticker_priority**
   - Priority system for quota-limited scenarios
   - Always process S&P 500 mega caps

### Key Views
- **v_assets_needing_processing** - Delta discovery (bulk_load, price_update, indicator_calc)
- **v_assets_needing_fundamental_refresh** - Smart fundamental refresh (NEW)
- **v_quota_status** - Real-time API quota status
- **v_recent_executions** - Last 7 days DAG runs
- **v_fundamental_refresh_summary** - Refresh schedule statistics

### Migrations
- `001_initial_schema.sql` - Base assets table
- `002_enhance_assets_table.sql` - Added 50+ fundamental data columns
- `003_technical_indicators_table.sql` - Technical analysis tables
- `004_add_etf_funds_and_partitioning.sql` - ETF support, partitioning strategy
- `005_process_metadata_tables.sql` - Airflow orchestration metadata
- `006_api_quota_management.sql` - API usage tracking (NEW - 2025-10-28)
- `007_smart_fundamental_refresh.sql` - Configurable refresh frequencies (NEW - 2025-10-28)

---

## API Quota Management

### EODHD API Pricing (All-World Plan: $79.99/month)
- **Daily Limit:** 100,000 API calls
- **Fundamentals API:** 10 calls per ticker
- **EOD Prices API:** 1 call per ticker
- **Bulk Exchange API:** 100 calls (not currently used)

### Quota Calculation
```python
# Before smart refresh (EXCEEDS LIMIT):
# 21,817 tickers × 11 calls (10 fundamentals + 1 price) = 239,987 calls/day ❌

# After smart refresh (UNDER LIMIT):
# 21,817 tickers × 1 call (price) = 21,817 calls/day
# ~3,117 tickers/week × 11 calls (fundamentals) = 4,898 calls/day
# Total: 26,715 calls/day ✅ (88.9% savings)
```

### Smart Fundamental Refresh Strategy
- **Mega Cap (>$200B):** Quarterly refresh - AAPL, MSFT, GOOGL (95% savings)
- **Large Cap ($10B-$200B):** Monthly refresh - Most S&P 500 (85% savings)
- **Mid Cap ($2B-$10B):** Weekly refresh (75% savings)
- **Small Cap ($300M-$2B):** Weekly refresh (75% savings)
- **Micro Cap (<$300M):** Weekly or Daily (high volatility)

### Priority System
When quota is limited (<20% remaining):
1. Always process: S&P 500 mega caps (priority level 1)
2. High priority: Large caps, high volume stocks (priority level 2)
3. Normal priority: Mid caps (priority level 3)
4. Low priority: Small caps (priority level 4)
5. Skip: Micro caps, low volume (priority level 5)

---

## Critical Architecture Decisions

### 1. BashOperator vs KubernetesPodOperator
**Decision:** Use BashOperator with kubectl commands instead of KubernetesPodOperator
**Reason:** KubernetesPodOperator has ARM64 compatibility issues in Airflow 3.x
- Tasks terminate after ~4 seconds despite pods running successfully
- BashOperator provides reliable task completion detection
- Direct pod lifecycle control with kubectl run/wait/logs/delete

**Implementation:** [dag_data_collection_equities.py:109-183](airflow/dags/data_loading/dag_data_collection_equities.py#L109-L183)

### 2. Smart Fundamental Refresh vs Daily Fetch
**Decision:** Configurable refresh frequencies (weekly/monthly/quarterly) instead of daily
**Reason:** Fundamental data changes infrequently (quarterly earnings cycles)
- Daily fetch wastes 218,170 API calls/day (10 calls × 21,817 tickers)
- 88.9% API cost reduction with weekly average
- Enables staying under 100K daily quota

**Implementation:** [007_smart_fundamental_refresh.sql](database/migrations/007_smart_fundamental_refresh.sql)

### 3. Metadata-Driven Delta Discovery
**Decision:** Use state machine (asset_processing_state) to track what needs processing
**Reason:** Avoid reprocessing unchanged data
- fundamentals_loaded flag prevents re-fetching bulk data
- prices_last_date enables incremental price updates (yesterday only)
- next_fundamental_refresh drives smart refresh schedule

**Implementation:** [v_assets_needing_processing view](database/migrations/007_smart_fundamental_refresh.sql#L100-L145)

### 4. Parallel Processing by Exchange
**Decision:** 4 parallel BashOperator jobs grouped by geographic region
**Reason:** Maximize throughput while respecting API rate limits
- US Markets (NYSE + NASDAQ): ~14,000 tickers
- London (LSE): ~2,000 tickers
- German Markets (Frankfurt + Xetra): ~1,500 tickers
- Other European (Euronext + BME + SIX): ~4,000 tickers

**Implementation:** [dag_data_collection_equities.py:186-250](airflow/dags/data_loading/dag_data_collection_equities.py#L186-L250)

### 5. TimescaleDB for Historical Prices
**Decision:** Use TimescaleDB hypertable with 7-day chunks
**Reason:** Optimize queries for time-series financial data
- Automatic partitioning by (ticker, time)
- Efficient queries for date ranges (last 90 days, YTD, etc.)
- Compression for older data (future enhancement)

**Implementation:** [003_technical_indicators_table.sql](database/migrations/003_technical_indicators_table.sql)

---

## Common Operations

### Database Operations

#### Connect to Database
```bash
# From WSL2 (requires tunnel running)
systemctl status postgres-tunnel.service  # Check tunnel status
psql -h localhost -U appuser -d appdb
```

#### Check Processing Status
```sql
SET search_path TO financial_screener;

-- Overall progress
SELECT * FROM get_processing_progress();

-- Assets needing processing
SELECT processing_need, COUNT(*)
FROM v_assets_needing_processing
GROUP BY processing_need;

-- Recent executions
SELECT * FROM v_recent_executions LIMIT 10;

-- API quota status
SELECT * FROM v_quota_status WHERE quota_date = CURRENT_DATE;

-- Fundamental refresh summary
SELECT * FROM v_fundamental_refresh_summary;
```

#### Mark Ticker for Reprocessing
```sql
-- Reprocess all data for ticker
SELECT reset_ticker_for_reprocessing('AAPL.US', 'all');

-- Reprocess only fundamentals
SELECT reset_ticker_for_reprocessing('AAPL.US', 'fundamentals');

-- Reprocess only prices
SELECT reset_ticker_for_reprocessing('AAPL.US', 'prices');
```

#### Set Fundamental Refresh Frequency
```sql
-- Set individual ticker to quarterly
SELECT set_fundamental_refresh_frequency('AAPL.US', 'quarterly');

-- Bulk set by market cap category
SELECT set_refresh_frequency_by_category('Mega', 'quarterly');
SELECT set_refresh_frequency_by_category('Large', 'monthly');
SELECT set_refresh_frequency_by_category('Mid', 'weekly');
```

### Airflow Operations

#### Trigger DAG Manually
```bash
# Via kubectl
kubectl exec -n airflow airflow-scheduler-0 -c scheduler -- \
  airflow dags trigger data_collection_equities

# Via web UI
# Navigate to http://192.168.1.240:30800
# Click "data_collection_equities" → "Trigger DAG"
```

#### Check DAG Status
```bash
# List all DAGs
kubectl exec -n airflow airflow-scheduler-0 -c scheduler -- \
  airflow dags list

# Show DAG details
kubectl exec -n airflow airflow-scheduler-0 -c scheduler -- \
  airflow dags show data_collection_equities

# View task instances
kubectl exec -n airflow airflow-scheduler-0 -c scheduler -- \
  airflow tasks list data_collection_equities
```

#### View Logs
```bash
# Airflow scheduler logs
kubectl logs -n airflow airflow-scheduler-0 -c scheduler --tail=100

# Data collector pod logs
POD_NAME=$(kubectl get pods -n financial-screener -l app=data-collector --no-headers | head -1 | awk '{print $1}')
kubectl logs -n financial-screener $POD_NAME --tail=100
```

### Kubernetes Operations

#### Check Cluster Status
```bash
# Node status
kubectl get nodes -o wide

# All pods
kubectl get pods --all-namespaces

# Financial screener namespace
kubectl get pods -n financial-screener -o wide

# Airflow namespace
kubectl get pods -n airflow -o wide

# Database namespace
kubectl get pods -n databases -o wide
```

#### Restart Airflow Components
```bash
# Restart scheduler
kubectl delete pod -n airflow airflow-scheduler-0

# Restart webserver
kubectl delete pod -n airflow -l component=webserver

# Restart dag-processor
kubectl delete pod -n airflow -l component=dag-processor
```

#### Clean Up Failed Pods
```bash
# Delete failed data collector pods
kubectl delete pods -n financial-screener -l app=data-collector --field-selector status.phase=Failed

# Delete completed pods
kubectl delete pods -n financial-screener -l app=data-collector --field-selector status.phase=Succeeded
```

### Data Collector Operations

#### Build and Distribute Docker Image
```bash
# Build on WSL2 (ARM64 emulation)
cd /root/gitlab/financial-screener
docker build -t financial-data-collector:latest -f services/data-collector/Dockerfile .

# Save and distribute to cluster nodes
/root/gitlab/financial-screener/scripts/build-and-distribute-data-collector.sh
```

#### Test Data Collector Locally
```bash
# Run with test dataset (10 tickers)
python3 services/data-collector/test_complete_data_collection.py

# Direct Python execution
cd services/data-collector
python3 src/main_enhanced.py \
  --execution-id "manual_test_$(date +%s)" \
  --exchanges "NYSE" \
  --mode auto \
  --batch-size 10 \
  --skip-existing
```

#### Monitor Running Jobs
```bash
# Watch pod status
watch -n 5 'kubectl get pods -n financial-screener -o wide'

# Follow logs in real-time
kubectl logs -n financial-screener -f -l app=data-collector

# Check pod resource usage
kubectl top pods -n financial-screener
```

---

## Troubleshooting Guide

### Issue: API Quota Exceeded (HTTP 402)
**Symptoms:**
```
402 Client Error: Payment Required for url: https://eodhd.com/api/fundamentals/...
```

**Diagnosis:**
```sql
-- Check today's quota status
SELECT * FROM v_quota_status WHERE quota_date = CURRENT_DATE;

-- Check API usage breakdown
SELECT * FROM v_api_usage_today;
```

**Solutions:**
1. **Wait for quota reset** (daily at 00:00 UTC)
2. **Reduce batch size** in Airflow DAG configuration
3. **Enable smart fundamental refresh** (migration 007)
4. **Use priority ticker system** (process only critical tickers)

### Issue: Database Connection Exhausted
**Symptoms:**
```
asyncpg.exceptions.TooManyConnectionsError: sorry, too many clients already
```

**Diagnosis:**
```bash
# Check running pods
kubectl get pods -n financial-screener

# Check active database connections
psql -h localhost -U appuser -d appdb -c \
  "SELECT count(*), state FROM pg_stat_activity GROUP BY state;"
```

**Solutions:**
```bash
# Clean up old running pods
kubectl delete pods -n financial-screener -l app=data-collector --field-selector status.phase=Running

# Increase PostgreSQL max_connections (if needed)
kubectl exec -n databases postgresql-primary-0 -- \
  psql -U appuser -d appdb -c "ALTER SYSTEM SET max_connections = 200;"
kubectl delete pod -n databases postgresql-primary-0  # Restart to apply
```

### Issue: Airflow DAG Not Showing Up
**Symptoms:**
DAG not visible in Airflow UI after deployment

**Diagnosis:**
```bash
# Check DAG file syntax
kubectl exec -n airflow airflow-scheduler-0 -c scheduler -- \
  python3 -c "import sys; sys.path.insert(0, '/opt/airflow/dags'); from data_loading.dag_data_collection_equities import dag; print(dag.dag_id)"

# Check scheduler logs for errors
kubectl logs -n airflow airflow-scheduler-0 -c scheduler --tail=100 | grep ERROR
```

**Solutions:**
```bash
# Re-copy DAG file to all Airflow components
scp airflow/dags/data_loading/dag_data_collection_equities.py admin@192.168.1.240:~/

kubectl cp ~/dag_data_collection_equities.py airflow/airflow-scheduler-0:/opt/airflow/dags/data_loading/ -c scheduler
kubectl cp ~/dag_data_collection_equities.py airflow/airflow-dag-processor-xxx:/opt/airflow/dags/data_loading/ -c dag-processor

# Restart Airflow components
kubectl delete pod -n airflow airflow-scheduler-0
kubectl delete pod -n airflow -l component=dag-processor
```

### Issue: BashOperator Task Fails
**Symptoms:**
Task marked as failed in Airflow UI

**Diagnosis:**
```bash
# Get task logs from Airflow
kubectl exec -n airflow airflow-scheduler-0 -c scheduler -- \
  airflow tasks logs data_collection_equities process_us_markets <run_id>

# Check if pod was created
kubectl get pods -n financial-screener -l app=data-collector

# Check pod logs (if still exists)
kubectl logs -n financial-screener <pod-name>
```

**Solutions:**
1. **Check pod creation** - Verify kubectl can create pods
2. **Check image availability** - Ensure financial-data-collector:latest exists on all nodes
3. **Check secrets** - Verify postgres-secret and data-api-secrets exist
4. **Check resource limits** - Ensure cluster has capacity (500m CPU, 1Gi RAM per pod)

### Issue: SSH Tunnel Not Working
**Symptoms:**
Cannot connect to PostgreSQL from WSL2

**Diagnosis:**
```bash
# Check tunnel service status
systemctl status postgres-tunnel.service

# Check if port 5432 is listening
nc -zv localhost 5432

# Check journal logs
journalctl -u postgres-tunnel.service -n 50
```

**Solutions:**
```bash
# Restart tunnel service
systemctl restart postgres-tunnel.service

# Manual tunnel (for debugging)
ssh -i ~/.ssh/pi_cluster -L 5432:postgresql-primary.databases.svc.cluster.local:5432 admin@192.168.1.240 -N &
```

---

## Performance Guidelines

### API Rate Limiting
- **EODHD Rate Limit:** No documented per-second limit, but recommend max 10 req/sec
- **Batch Size:** 500 tickers per job (can be adjusted based on quota)
- **Parallel Jobs:** 4 simultaneous (one per market region)
- **Retry Strategy:** 3 retries with exponential backoff (5s, 15s, 45s)

### Database Optimization
- **Indexes:** All foreign keys and common query fields indexed
- **Partitioning:** TimescaleDB automatic partitioning (7-day chunks)
- **Connection Pooling:** Each pod uses max 5 connections
- **Batch Inserts:** 100 rows per INSERT (historical prices)

### Kubernetes Resource Allocation
```yaml
resources:
  requests:
    cpu: "500m"      # 0.5 cores minimum
    memory: "1Gi"    # 1GB minimum
  limits:
    cpu: "2000m"     # 2 cores maximum
    memory: "2Gi"    # 2GB maximum
```

### Expected Performance
- **Day 1 (Full Load):** 4-6 hours for 21,817 tickers (parallel processing)
- **Steady State (Daily):** 30-60 minutes for price updates only
- **API Calls:**
  - Day 1: ~93,000 calls (with smart refresh)
  - Daily: ~26,000 calls (prices + weekly fundamental batch)

---

## Security Notes

### Secrets Management
All secrets stored in Kubernetes secrets (not in code):
```bash
# Database connection
kubectl get secret postgres-secret -n financial-screener -o yaml

# API keys
kubectl get secret data-api-secrets -n financial-screener -o yaml
```

### SSH Keys
- **Cluster Access:** ~/.ssh/pi_cluster (private key, 600 permissions)
- **Git Access:** ~/.ssh/id_rsa (if using private repo)

### Network Security
- **Cluster Network:** 192.168.1.0/24 (private LAN)
- **No External Exposure:** All services accessed via SSH tunnel or NodePort internally
- **Database:** Only accessible within cluster or via SSH tunnel

---

## Known Issues & Workarounds

### 1. KubernetesPodOperator ARM64 Incompatibility
**Issue:** Tasks terminate prematurely on ARM64 with Airflow 3.x
**Workaround:** Use BashOperator with kubectl commands
**Status:** Permanent solution implemented (2025-10-28)
**Reference:** [AIRFLOW_MIGRATION_STATUS.md](docs/AIRFLOW_MIGRATION_STATUS.md)

### 2. API Quota Exhaustion During Testing
**Issue:** Multiple test runs consumed 150K+ API calls in one day
**Workaround:** Implemented smart fundamental refresh + quota tracking
**Status:** Fixed with migrations 006 and 007 (2025-10-28)
**Reference:** [006_api_quota_management.sql](database/migrations/006_api_quota_management.sql)

### 3. Git-Sync Not Configured
**Issue:** DAGs must be manually copied to Airflow pods
**Workaround:** Manual scp + kubectl cp deployment
**Status:** Config file exists but not deployed ([airflow-git-sync-config.yaml](kubernetes/airflow-git-sync-config.yaml))
**Future:** Configure git-sync sidecar for automatic DAG deployment

### 4. Ticker List Fragmentation
**Issue:** Initial ticker lists were by index (sp500.txt, ftse100.txt) but data is grouped by exchange
**Solution:** Rebuilt lists by exchange (nyse.txt, nasdaq.txt, lse.txt, etc.)
**Status:** Resolved (2025-10-22)
**Reference:** [config/tickers/README.md](config/tickers/README.md)

---

## Development Workflow

### Making Changes

#### 1. Update DAG Files
```bash
# Edit locally
vim /root/gitlab/financial-screener/airflow/dags/data_loading/dag_data_collection_equities.py

# Test syntax locally
python3 -m py_compile airflow/dags/data_loading/dag_data_collection_equities.py

# Deploy to cluster
scp airflow/dags/data_loading/dag_data_collection_equities.py admin@192.168.1.240:~/

# Copy to all Airflow components
kubectl cp ~/dag_data_collection_equities.py airflow/airflow-scheduler-0:/opt/airflow/dags/data_loading/ -c scheduler
kubectl cp ~/dag_data_collection_equities.py airflow/airflow-dag-processor-xxx:/opt/airflow/dags/data_loading/ -c dag-processor

# Verify DAG loaded
kubectl exec -n airflow airflow-scheduler-0 -c scheduler -- \
  airflow dags list | grep data_collection_equities
```

#### 2. Update Database Schema
```bash
# Create migration file
vim database/migrations/008_new_feature.sql

# Apply migration
psql -h localhost -U appuser -d appdb -f database/migrations/008_new_feature.sql

# Verify changes
psql -h localhost -U appuser -d appdb -c "\dt financial_screener.*"
```

#### 3. Update Data Collector Code
```bash
# Edit source
vim services/data-collector/src/main_enhanced.py

# Test locally (with sample dataset)
cd services/data-collector
python3 src/main_enhanced.py --execution-id "test_$(date +%s)" --exchanges "NYSE" --batch-size 10

# Build new Docker image
cd /root/gitlab/financial-screener
docker build -t financial-data-collector:latest -f services/data-collector/Dockerfile .

# Distribute to cluster nodes
./scripts/build-and-distribute-data-collector.sh

# Verify image on nodes
for i in {240..247}; do
  echo "=== Node 192.168.1.$i ==="
  ssh -i ~/.ssh/pi_cluster admin@192.168.1.$i "sudo crictl images | grep financial-data-collector"
done
```

### Testing

#### Unit Tests
```bash
# Test data collector components
cd services/data-collector
python3 -m pytest tests/

# Test database utilities
cd airflow/dags/utils
python3 database_utils.py  # Runs test_connection()
python3 api_quota_calculator.py  # Runs test suite
```

#### Integration Tests
```bash
# Test complete data collection (10 tickers)
python3 services/data-collector/test_complete_data_collection.py

# Test Airflow DAG (dry run)
kubectl exec -n airflow airflow-scheduler-0 -c scheduler -- \
  airflow dags test data_collection_equities 2025-10-28
```

### Git Workflow
```bash
# Stage changes
git add database/migrations/007_smart_fundamental_refresh.sql
git add airflow/dags/utils/api_quota_calculator.py

# Commit with descriptive message
git commit -m "Add smart fundamental refresh and API quota management

- Migration 007: Configurable refresh frequencies (weekly/monthly/quarterly)
- Migration 006: API usage tracking and quota management
- API quota calculator utility with intelligent batch adjustment
- Updated metadata_helpers.py to use new quota system
- 88.9% reduction in API calls (239K → 27K per day)
"

# Push to remote
git push origin main
```

---

## Monitoring & Alerts

### Key Metrics to Monitor

#### 1. API Quota Health
```sql
-- Real-time quota status
SELECT
  calls_used,
  calls_remaining,
  usage_percentage,
  quota_health,
  quota_exhausted
FROM v_quota_status
WHERE quota_date = CURRENT_DATE;
```

**Alert Thresholds:**
- Warning: >70% usage (70,000+ calls)
- Critical: >90% usage (90,000+ calls)
- Emergency: Quota exhausted (100,000 calls)

#### 2. DAG Execution Success Rate
```sql
-- Last 7 days success rate
SELECT
  process_name,
  COUNT(*) as total_runs,
  COUNT(*) FILTER (WHERE status = 'success') as successful,
  COUNT(*) FILTER (WHERE status = 'failed') as failed,
  ROUND(100.0 * COUNT(*) FILTER (WHERE status = 'success') / COUNT(*), 2) as success_rate
FROM process_executions
WHERE started_at > NOW() - INTERVAL '7 days'
GROUP BY process_name;
```

**Alert Thresholds:**
- Warning: <90% success rate
- Critical: <80% success rate

#### 3. Data Freshness
```sql
-- Check stale price data
SELECT
  COUNT(*) as stale_tickers,
  MAX(prices_last_date) as most_recent_update
FROM asset_processing_state
WHERE prices_last_date < CURRENT_DATE - 2
  AND consecutive_failures < 5;
```

**Alert Thresholds:**
- Warning: >100 tickers with stale data (>2 days old)
- Critical: >1000 tickers with stale data

#### 4. Cluster Health
```bash
# Node status
kubectl get nodes -o wide

# Pod status by namespace
kubectl get pods --all-namespaces -o wide

# Resource usage
kubectl top nodes
kubectl top pods -n financial-screener
```

**Alert Thresholds:**
- Warning: Node memory >80%
- Critical: Node memory >95%
- Critical: Pod restart count >5

---

## Future Enhancements

### Phase 3: Technical Analysis
- Implement indicator calculation DAG
- Calculate SMA, EMA, RSI, MACD, Bollinger Bands
- Store in technical_indicators table
- Schedule: Run after price updates complete

### Phase 4: Screening Engine
- Build screening API (FastAPI)
- Query language for complex filters
- Pre-calculated common screens (e.g., "High momentum stocks")
- REST API + GraphQL endpoints

### Phase 5: Web Frontend
- React dashboard for browsing stocks
- Interactive charts (TradingView integration)
- Watchlist management
- Custom screen builder

### Phase 6: Advanced Features
- Real-time price updates (WebSocket integration)
- News sentiment analysis
- Options data integration
- Portfolio tracking
- Backtesting engine

---

## References

### Documentation
- [AIRFLOW_MIGRATION_STATUS.md](docs/AIRFLOW_MIGRATION_STATUS.md) - Airflow 3.x migration details
- [API_USAGE_DEEP_INVESTIGATION.md](docs/API_USAGE_DEEP_INVESTIGATION.md) - API quota analysis
- [EODHD_DATA_COVERAGE.md](docs/EODHD_DATA_COVERAGE.md) - EODHD API capabilities
- [PHASE1_API_USAGE_ANALYSIS.md](docs/PHASE1_API_USAGE_ANALYSIS.md) - Initial API testing results
- [REVISED_LOADING_STRATEGY.md](docs/REVISED_LOADING_STRATEGY.md) - Data collection strategy
- [config/tickers/README.md](config/tickers/README.md) - Ticker list documentation

### External Resources
- [EODHD API Documentation](https://eodhd.com/financial-apis/)
- [Apache Airflow 3.x Docs](https://airflow.apache.org/docs/apache-airflow/stable/)
- [TimescaleDB Best Practices](https://docs.timescale.com/timescaledb/latest/overview/core-concepts/hypertables-and-chunks/)
- [K3s ARM64 Docs](https://docs.k3s.io/installation/requirements#operating-systems)

---

## Contact & Support

**Project Owner:** Financial Screener Team
**Repository:** /root/gitlab/financial-screener
**Cluster Location:** Home Lab - Raspberry Pi 4 Cluster (8 nodes)
**Last Major Update:** 2025-10-28 (API Quota Management + Smart Fundamental Refresh)

For issues or questions, check:
1. This .claude.md file first
2. Relevant documentation in [docs/](docs/)
3. Database migration files in [database/migrations/](database/migrations/)
4. Airflow DAG documentation in [airflow/dags/](airflow/dags/)
