"""
Data Collection DAG - Equities
Orchestrates daily data collection with metadata-driven delta discovery

Schedule: Daily at 21:30 UTC (after US market close)
Features:
  - Delta discovery from metadata tables
  - Parallel processing by exchange
  - API quota management
  - Automatic retry logic
  - Complete execution tracking
"""

from airflow import DAG
from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator
from airflow.providers.cncf.kubernetes.secret import Secret
from airflow.operators.python import PythonOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from datetime import datetime, timedelta
from kubernetes.client import models as k8s
import sys
import os

# Add utils to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from utils.metadata_helpers import (
    initialize_execution_record,
    discover_processing_delta,
    check_and_adjust_quota,
    finalize_execution_record,
)

# =============================================================================
# DAG CONFIGURATION
# =============================================================================

default_args = {
    'owner': 'financial-screener',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'retry_exponential_backoff': True,
    'max_retry_delay': timedelta(minutes=30),
}

dag = DAG(
    dag_id='data_collection_equities',
    default_args=default_args,
    description='Daily equities data collection with parallel processing',
    schedule='30 21 * * *',  # 21:30 UTC (after US market close) - Airflow 3.0 uses 'schedule' not 'schedule_interval'
    start_date=datetime(2025, 10, 21),
    catchup=False,
    max_active_runs=1,
    tags=['data-collection', 'equities', 'daily', 'production'],
    doc_md=__doc__,
)

# =============================================================================
# KUBERNETES SECRETS
# =============================================================================

# PostgreSQL connection
db_secret = Secret(
    deploy_type='env',
    deploy_target='DATABASE_URL',
    secret='postgres-secret',
    key='DATABASE_URL'
)

# EODHD API key
api_secret = Secret(
    deploy_type='env',
    deploy_target='EODHD_API_KEY',
    secret='data-api-secrets',
    key='EODHD_API_KEY'
)

# =============================================================================
# TASK DEFINITIONS
# =============================================================================

# Task 1: Initialize execution record in metadata
initialize_task = PythonOperator(
    task_id='initialize_execution',
    python_callable=initialize_execution_record,
    dag=dag,
    doc_md="""
    Creates a record in process_executions table to track this DAG run.
    Returns execution_id (Airflow run_id) for downstream tasks.
    """
)

# Task 2: Discover what needs processing
discover_task = PythonOperator(
    task_id='discover_delta',
    python_callable=discover_processing_delta,
    dag=dag,
    doc_md="""
    Queries v_assets_needing_processing view to determine:
    - New tickers requiring bulk load (fundamentals + 2 years prices)
    - Existing tickers requiring price updates (yesterday's price)
    - Tickers requiring indicator recalculation

    Results stored in XCom for downstream tasks.
    """
)

# Task 3: Check API quota and adjust batch sizes
quota_check_task = PythonOperator(
    task_id='check_api_quota',
    python_callable=check_and_adjust_quota,
    dag=dag,
    doc_md="""
    Calculates required API calls based on delta discovery.
    Adjusts batch sizes if needed to stay within daily quota (100K calls).

    API costs:
    - New ticker (bulk load): 11 calls (10 fundamentals + 1 prices)
    - Existing ticker (price update): 1 call
    """
)

# =============================================================================
# PARALLEL PROCESSING JOBS (BY EXCHANGE)
# =============================================================================

# Common Kubernetes pod configuration
from airflow.providers.cncf.kubernetes.utils.pod_manager import OnFinishAction

common_pod_config = {
    'namespace': 'financial-screener',
    'image': 'financial-data-collector:latest',
    'image_pull_policy': 'Never',  # Use local images distributed via tar
    'cmds': ['python', '/app/src/main_enhanced.py'],
    'secrets': [db_secret, api_secret],
    'get_logs': True,
    'do_xcom_push': False,  # Don't try to push logs to XCom
    'on_finish_action': OnFinishAction.DELETE_SUCCEEDED_POD,  # Delete only on success
    'in_cluster': True,
    'service_account_name': 'default',
    'startup_timeout_seconds': 600,  # 10 minutes for pod startup
    'poll_interval': 10,  # Check pod status every 10 seconds
    'container_resources': k8s.V1ResourceRequirements(
        requests={"cpu": "500m", "memory": "1Gi"},
        limits={"cpu": "2000m", "memory": "2Gi"}
    ),
}

# Job 1: US Markets (NYSE + NASDAQ)
# Processes both new assets (bulk) and price updates (incremental)
us_markets_job = KubernetesPodOperator(
    task_id='process_us_markets',
    name='data-collector-us-{{ run_id | replace("_", "-") | replace(":", "-") | replace("+", "-") | replace(".", "-") | lower }}',
    arguments=[
        '--execution-id', '{{ run_id }}',
        '--exchanges', 'NYSE,NASDAQ',
        '--mode', 'auto',  # Auto-detects bulk vs incremental per ticker
        '--skip-existing',
        '--batch-size', '500',
    ],
    labels={
        'app': 'data-collector',
        'market': 'us',
        'dag_id': '{{ dag.dag_id }}',
        'task_id': 'process_us_markets',
    },
    dag=dag,
    **common_pod_config
)

# Job 2: London Stock Exchange
lse_job = KubernetesPodOperator(
    task_id='process_lse',
    name='data-collector-lse-{{ run_id | replace("_", "-") | replace(":", "-") | replace("+", "-") | replace(".", "-") | lower }}',
    arguments=[
        '--execution-id', '{{ run_id }}',
        '--exchanges', 'LSE',
        '--mode', 'auto',
        '--skip-existing',
        '--batch-size', '500',
    ],
    labels={
        'app': 'data-collector',
        'market': 'uk',
        'dag_id': '{{ dag.dag_id }}',
        'task_id': 'process_lse',
    },
    dag=dag,
    **common_pod_config
)

# Job 3: German Markets (Frankfurt + Xetra)
german_markets_job = KubernetesPodOperator(
    task_id='process_german_markets',
    name='data-collector-de-{{ run_id | replace("_", "-") | replace(":", "-") | replace("+", "-") | replace(".", "-") | lower }}',
    arguments=[
        '--execution-id', '{{ run_id }}',
        '--exchanges', 'FRANKFURT,XETRA',
        '--mode', 'auto',
        '--skip-existing',
        '--batch-size', '500',
    ],
    labels={
        'app': 'data-collector',
        'market': 'germany',
        'dag_id': '{{ dag.dag_id }}',
        'task_id': 'process_german_markets',
    },
    dag=dag,
    **common_pod_config
)

# Job 4: Other European Markets (Euronext, BME, SIX)
european_markets_job = KubernetesPodOperator(
    task_id='process_european_markets',
    name='data-collector-eu-{{ run_id | replace("_", "-") | replace(":", "-") | replace("+", "-") | replace(".", "-") | lower }}',
    arguments=[
        '--execution-id', '{{ run_id }}',
        '--exchanges', 'EURONEXT,BME,SIX',
        '--mode', 'auto',
        '--skip-existing',
        '--batch-size', '500',
    ],
    labels={
        'app': 'data-collector',
        'market': 'europe',
        'dag_id': '{{ dag.dag_id }}',
        'task_id': 'process_european_markets',
    },
    dag=dag,
    **common_pod_config
)

# =============================================================================
# FINALIZATION TASKS
# =============================================================================

# Task: Finalize execution record
finalize_task = PythonOperator(
    task_id='finalize_execution',
    python_callable=finalize_execution_record,
    dag=dag,
    doc_md="""
    Aggregates results from asset_processing_details table.
    Updates process_executions record with:
    - Total assets processed
    - Success/failure counts
    - Total API calls used
    - Final status (success/partial/failed)
    """
)

# Task: Trigger indicator calculation DAG
trigger_indicators_task = TriggerDagRunOperator(
    task_id='trigger_indicators',
    trigger_dag_id='calculate_indicators',
    wait_for_completion=False,
    dag=dag,
    doc_md="""
    Triggers the indicator calculation DAG to process assets with updated prices.
    Runs asynchronously (does not wait for completion).
    """
)

# =============================================================================
# TASK DEPENDENCIES
# =============================================================================

# Linear dependency chain for initialization
initialize_task >> discover_task >> quota_check_task

# Parallel processing jobs (run simultaneously)
quota_check_task >> [
    us_markets_job,
    lse_job,
    german_markets_job,
    european_markets_job
]

# All jobs must complete before finalization
[
    us_markets_job,
    lse_job,
    german_markets_job,
    european_markets_job
] >> finalize_task

# Trigger indicators after finalization
finalize_task >> trigger_indicators_task

# =============================================================================
# TASK FLOW DIAGRAM
# =============================================================================

"""
Task Flow:

initialize_execution
    ↓
discover_delta (queries metadata)
    ↓
check_api_quota (adjusts batch sizes)
    ↓
    ├─→ process_us_markets (NYSE, NASDAQ)
    ├─→ process_lse (LSE)
    ├─→ process_german_markets (Frankfurt, Xetra)
    └─→ process_european_markets (Euronext, BME, SIX)
    ↓
finalize_execution (update metadata)
    ↓
trigger_indicators (start next DAG)

Expected Duration:
- Day 1 (catch-up): 4-6 hours (parallel processing)
- Steady state: 2-3 hours (price updates only)

API Usage:
- Day 1: ~93,000 calls (5,045 updates + 8,000 new assets)
- Steady state: ~22,000 calls (21,817 price updates)
"""
